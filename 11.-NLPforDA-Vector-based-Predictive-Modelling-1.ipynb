{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing for Data Analytics\n",
    "[The Analytics Store](http://www.theanalyticsstore.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Predictive Modelling With Vector Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lots of Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build predictive models in Python we use a set of libraries that are imported here. In particular **pandas** and **sklearn** are particularly important. Plus nltk for data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally we need to install bits and pieces (e.g. corpora) from the NLTK. To do this uncommment and run the code below which will launch the interactive NLTK downloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this in order to launch the NLTK downloader to access corpora, packages etc\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load an nltk corpus, pre-process it and the us a scikit-learn vectoriser to convert it to a format appropriate for ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare nltk Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a corpus object from a collection of text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_grps_20_corpus = nltk.corpus.CategorizedPlaintextCorpusReader(root = './data/20news-bydate/20news-bydate-train/', \\\n",
    "                                                                   fileids = r'.+/\\d+', \\\n",
    "                                                                   cat_pattern=r'((\\w|\\.)+)/*', \\\n",
    "                                                                   encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a pre-processing function that perfoms text cleaning and normalisation operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(words, to_lowercase = True, remove_punctuation = True, remove_digits = True, remove_odd_chars = True, remove_stopwords=True, stem = True):\n",
    "    if to_lowercase:\n",
    "        words = [w.lower() for w in words]\n",
    "    \n",
    "    if remove_punctuation:\n",
    "        words = [w for w in words if not (re.match(r'^\\W+$', w) != None)]\n",
    "    \n",
    "    if remove_digits:\n",
    "        words = [w for w in words if not w.replace('.','',1).isdigit()]\n",
    "\n",
    "    if remove_odd_chars:\n",
    "        words = [re.sub(r'[^a-zA-Z0-9_]','_', w) for w in words]\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        sw = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in sw]\n",
    "\n",
    "    if stem:\n",
    "        porter = nltk.PorterStemmer()\n",
    "        words = [porter.stem(w) for w in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate across the corpus applying pre-processing and storing text data in a list with file id and category meta-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [((fileid, category), preprocess(news_grps_20_corpus.words(fileid), to_lowercase = True, remove_punctuation = True, remove_digits = True, remove_odd_chars = True, remove_stopwords=True, stem = False)) \\\n",
    "#             for category in news_grps_20_corpus.categories() \\\n",
    "             for category in ['alt.atheism', 'comp.graphics'] \\\n",
    "             for fileid in news_grps_20_corpus.fileids(category)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Vector Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a dummy function to stop the scikit-learn vecrtorisers performing tokenisation and other pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a CountVectoriser object rady to perfrom the transformation - use the dummy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_gen = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df = 100,\n",
    "    max_df = 0.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the texts into a bag of words represnetation and add fileid and category meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = bow_gen.fit_transform([doc[1] for doc in documents])\n",
    "fileids = [doc[0][0] for doc in documents]\n",
    "cats = [doc[0][1] for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a nice pandas data frame containing all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_df = pd.DataFrame(bow.toarray(), columns = bow_gen.get_feature_names())\n",
    "bow_df['fileids'] = fileids\n",
    "bow_df['target'] = cats\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same preparation but with a tf-idf vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_gen = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None) \n",
    "tfidf = tfidf_gen.fit_transform([doc[1] for doc in documents])\n",
    "fileids = [doc[0][0] for doc in documents]\n",
    "cats = [doc[0][1] for doc in documents]\n",
    "tfidf_df = pd.DataFrame(tfidf.toarray(), columns = tfidf_gen.get_feature_names())\n",
    "tfidf_df['fileids'] = fileids\n",
    "tfidf_df['target'] = cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select either tf-idf or bow representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the distribution of the classification targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[dataset.columns.difference(['fileids', 'target'])]\n",
    "y = dataset['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a **training set**, a **validation set**, and a **test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test \\\n",
    "    = sklearn.model_selection.train_test_split(X, y, random_state=0, \\\n",
    "                                    train_size = 0.7)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid \\\n",
    "    = sklearn.model_selection.train_test_split(X_train_plus_valid, \\\n",
    "                                        y_train_plus_valid, \\\n",
    "                                        random_state=0, \\\n",
    "                                        train_size = 0.5/0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Naive Bayes classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_model = sklearn.naive_bayes.MultinomialNB()\n",
    "my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the decision tree on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model.predict(X_train)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = sklearn.metrics.accuracy_score(y_train, y_pred) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(sklearn.metrics.classification_report(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_train, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the tree on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = sklearn.metrics.accuracy_score(y_valid, y_pred) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print nicer confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Parameters Using a Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a cross validation to perfrom an evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_model = sklearn.naive_bayes.MultinomialNB()\n",
    "scores = sklearn.model_selection.cross_val_score(my_model, X_train_plus_valid, y_train_plus_valid, cv=5)\n",
    "print(scores)\n",
    "print(np.mean(scores), \"+/-\", np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a grid search through a large set of possible parameters. Here we try depths between 3 and 20 and different limits on the minimum number of samples per split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid ={'fit_prior': [True, False], \\\n",
    "             'alpha': list(np.arange(0.1, 1.1, 0.1))}\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_model = sklearn.model_selection.GridSearchCV(sklearn.naive_bayes.MultinomialNB(), \\\n",
    "                                param_grid, cv=2, verbose = 2)\n",
    "my_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "display(my_tuned_model.best_params_)\n",
    "display(my_tuned_model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on a stratified test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tuned_model.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily use the same patterns to train other types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the same job with random forests\n",
    "my_model = sklearn.ensemble.RandomForestClassifier(n_estimators=300, \\\n",
    "                                           max_features = 3,\\\n",
    "                                           min_samples_split=200)\n",
    "my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the model on the **validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose parameters using a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = [\n",
    " {'n_estimators': list(range(100, 501, 50)), 'max_features': list(range(1, 10, 2)), 'min_samples_split': list(range(20, 200, 50)) }\n",
    "]\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_model = sklearn.model_selection.GridSearchCV(sklearn.ensemble.RandomForestClassifier(), param_grid, cv=5)\n",
    "my_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(my_tuned_model.best_params_)\n",
    "print(my_tuned_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tuned_model.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same job with random forests\n",
    "my_model = sklearn.ensemble.BaggingClassifier(base_estimator = sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 50), \\\n",
    "                                      n_estimators=10)\n",
    "my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = [\n",
    " {'n_estimators': list(range(5, 25, 1))}\n",
    "]\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_model = sklearn.model_selection.GridSearchCV(sklearn.ensemble.BaggingClassifier(), param_grid, cv=5)\n",
    "my_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(my_tuned_model.best_params_)\n",
    "print(my_tuned_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tuned_model.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same job with random forests\n",
    "my_model = sklearn.ensemble.AdaBoostClassifier(base_estimator = sklearn.tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 50), \\\n",
    "                                       n_estimators=10)\n",
    "my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the same job with logistic regression\n",
    "my_model = sklearn.linear_model.LogisticRegression()\n",
    "my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the model on the **validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do the same job with random forests\n",
    "my_model = sklearn.neighbors.KNeighborsClassifier()\n",
    "my_model = my_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the decision tree on the **validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose parameters using a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = [\n",
    " {'n_neighbors':[3,5,15, 25]}\n",
    "]\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_model = sklearn.model_selection.GridSearchCV(sklearn.neighbors.KNeighborsClassifier(), param_grid, cv=5)\n",
    "my_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(my_tuned_model.best_params_)\n",
    "print(my_tuned_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tuned_model.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
